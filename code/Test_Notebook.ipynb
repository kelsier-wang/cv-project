{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl (30.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/30.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/kelsierwang/miniconda3/lib/python3.9/site-packages (from opencv-python) (1.23.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import (Conv2D, MaxPool2D, Dropout, Flatten, \n",
    "    Dense, BatchNormalization, RandomFlip, RandomContrast, RandomRotation, RandomZoom,LeakyReLU)\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import preprocessing\n",
    "from model import CNN_Model\n",
    "import recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TODO: change path to absolute path of data/images folder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_data, testing_data, training_labels, testing_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_data(testing_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, image_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, image_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/juliannerudner/Desktop/CS1430/CS1430_Final_Project/data/images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: change path to absolute path of data/images folder\n",
    "training_data, testing_data, training_labels, testing_labels = preprocessing.generate_data(testing_size = 0.2, image_height=256, image_width=256, path=\"/Users/juliannerudner/Desktop/CS1430/CS1430_Final_Project/data/images\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 10:47:42.602525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = CNN_Model()\n",
    "architecture = Sequential([\n",
    "                Conv2D(16, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Conv2D(16, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Conv2D(32, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Conv2D(32, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Dropout(0.1), \n",
    "                Conv2D(64, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Conv2D(64, (2,2), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                BatchNormalization(),\n",
    "                MaxPool2D(pool_size=(2,2)),\n",
    "                Flatten(),\n",
    "                Dense(128, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dense(64, activation='relu'),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.1), \n",
    "                Dense(21, activation=\"softmax\")\n",
    "            ])\n",
    "checkpoint_path = \"training_1/cp.ckpt\" #TODO: change to desired path\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "os.listdir(checkpoint_dir)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model.build_model(architecture=architecture, optimizer=Adam, learning_rate=0.002)\n",
    "# model.train(train_data=training_data,train_labels=training_labels,augment=False,epochs=20,batch_size=10,callbacks=cp_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 426ms/step - loss: 0.5957 - sparse_categorical_accuracy: 0.8063\n",
      "done testing\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"checkpoints/acc_65/cp.ckpt\" #TODO: change to desired path\n",
    "model.load_state(checkpoint_path)\n",
    "test_loss, test_accuracy = model.test(test_data=testing_data,test_labels=testing_labels)\n",
    "print(\"done testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a method for retrieving the predicted classes (by id) after training the model. We can use this to compare to the actual labels / retrieve recall information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 3s 177ms/step\n",
      "[ 0  0 17 17  9 12  1 11  2 18  2 18 16  3  3  3  4 11  4  4  4 18 13  6\n",
      " 17  6  6  6  6  6 18  6  6  7  7  7  7 11 17 20  8  9 20  6  9 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10  6 10 10 11 11 11 11 11 11\n",
      " 11 18 11 11 11 11 16 12  6 11 12 13 13 13 13  6 13 14 14 14 14 14 14 14\n",
      " 14 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 17 17 17 17 17 17 17  6 17 11 18 17 17  7 17 17 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 19 13 17 19 19 20 20 20 11 20 11 20\n",
      " 20 11 20 20 20 20 20 20 11 20 20 20 20 11 20 20 11 20 20 20 20 20 20]\n",
      "[ 0  0  0  0  1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  4  5  5  5\n",
      "  5  6  6  6  6  6  6  6  6  7  7  7  7  8  8  8  8  9  9  9  9 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 12 12 12 12 13 13 13 13 13 13 14 14 14 14 14 14 14\n",
      " 14 15 15 15 15 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 20 20 20 20 20 20 20\n",
      " 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      "0.806282722513089\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = np.argmax(model.model.predict(testing_data,batch_size=15),axis=1)\n",
    "print(predicted_classes)\n",
    "print(testing_labels)\n",
    "def acc(predicted_classes, testing_labels):\n",
    "    a = 0\n",
    "    for i in range(len(predicted_classes)):\n",
    "        if predicted_classes[i]==testing_labels[i]:\n",
    "            a+=1\n",
    "    acc = a / len(predicted_classes)\n",
    "    return acc\n",
    "\n",
    "print(acc(predicted_classes, testing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for seeing if any of the identified medications have been recalled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = recall.loaddata()\n",
    "for pill in predicted_classes:\n",
    "    is_recalled = data[' Recalled'][pill] #TODO: replace this with the recall.recall function\n",
    "    if is_recalled:\n",
    "        print(data['Name'][pill])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f997d85fc99c2cd87f0e019fe40484e1df3299f74c9cb9ff90e7d6c5b5906a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
